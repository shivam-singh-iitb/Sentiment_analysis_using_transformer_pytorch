{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzFV_20OfZ61"
   },
   "source": [
    "# Sentiment Analysis in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  9 06:27:21 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:19:00.0 Off |                  N/A |\r\n",
      "| 40%   65C    P2    88W / 250W |  10791MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\r\n",
      "| 36%   55C    P8    10W / 250W |     12MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:67:00.0 Off |                  N/A |\r\n",
      "| 39%   61C    P8     1W / 250W |     12MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:68:00.0 Off |                  N/A |\r\n",
      "| 30%   47C    P8    15W / 250W |     72MiB / 11018MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      3626      C   python3                                    10779MiB |\r\n",
      "|    3      1531      G   /usr/lib/xorg/Xorg                            59MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkCY1KUffZ62",
    "outputId": "d450282b-ae85-4c17-ca31-572efc0ee96a"
   },
   "outputs": [],
   "source": [
    "#library imports\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "\n",
    "import pickle\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "# PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma6AmTBsfZ65"
   },
   "source": [
    "## Multiclass Text Classification\n",
    "\n",
    "We are going to predict item ratings based on customer reviews bsed on this dataset from Kaggle:\n",
    "https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyGy_mCkOBX8",
    "outputId": "7572a53e-922d-4958-9358-d8760758eb26"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "XmGSB3yEfZ65",
    "outputId": "29193239-f405-431a-b4a4-325864196b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This book was very informative, covering all a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I didn't like this product it smudged all unde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I simply love the product. I appreciate print ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>It goes on very easily and makes my eyes look ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            reviews  ratings\n",
       "0           0  This book was very informative, covering all a...        4\n",
       "1           1  I am already a baseball fan and knew a bit abo...        5\n",
       "2           2  I didn't like this product it smudged all unde...        1\n",
       "3           3  I simply love the product. I appreciate print ...        5\n",
       "4           4  It goes on very easily and makes my eyes look ...        5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data\n",
    "# reviews = pd.read_csv(\"/content/drive/MyDrive/data/NLP_sentiment_analysis_data/train.csv\")\n",
    "reviews = pd.read_csv(\"train.csv\")\n",
    "print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d3uldQiTfZ65"
   },
   "outputs": [],
   "source": [
    "# reviews['Title'] = reviews['Title'].fillna('')\n",
    "# reviews['Review Text'] = reviews['Review Text'].fillna('')\n",
    "# reviews['review'] = reviews['Title'] + ' ' + reviews['Review Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "Oh60NcTwfZ65",
    "outputId": "018f219a-18b5-4ebb-ed91-c1c1c49814d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This book was very informative, covering all a...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I didn't like this product it smudged all unde...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I simply love the product. I appreciate print ...</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It goes on very easily and makes my eyes look ...</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  ratings  review_length\n",
       "0  This book was very informative, covering all a...        4             10\n",
       "1  I am already a baseball fan and knew a bit abo...        5             23\n",
       "2  I didn't like this product it smudged all unde...        1             14\n",
       "3  I simply love the product. I appreciate print ...        5             13\n",
       "4  It goes on very easily and makes my eyes look ...        5             13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keeping only relevant columns and calculating sentence lengths\n",
    "reviews = reviews[['reviews', 'ratings']]\n",
    "reviews.columns = ['reviews', 'ratings']\n",
    "reviews['review_length'] = reviews['reviews'].apply(lambda x: len(x.split()))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "TvsOZQR8fZ66",
    "outputId": "718a39ce-9af3-4e10-f8d8-da2fdb2d3a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This book was very informative, covering all a...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I didn't like this product it smudged all unde...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I simply love the product. I appreciate print ...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It goes on very easily and makes my eyes look ...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  ratings  review_length\n",
       "0  This book was very informative, covering all a...        3             10\n",
       "1  I am already a baseball fan and knew a bit abo...        4             23\n",
       "2  I didn't like this product it smudged all unde...        0             14\n",
       "3  I simply love the product. I appreciate print ...        4             13\n",
       "4  It goes on very easily and makes my eyes look ...        4             13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing ratings to 0-numbering\n",
    "zero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "reviews['ratings'] = reviews['ratings'].apply(lambda x: zero_numbering[x])\n",
    "print(type(reviews['ratings']))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for repeatability\n",
    "def Random(seed_value):\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "#     import os\n",
    "#     os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    import numpy as np\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "#     # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "#     import tensorflow as tf\n",
    "#     tf.random.set_seed(seed_value)\n",
    "\n",
    "Random(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-BYV0shfZ66",
    "outputId": "348fa896-b057-42f9-c561-38bb5ab093f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.58756"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean sentence length\n",
    "np.mean(reviews['review_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uPS7xmtFfZ66"
   },
   "outputs": [],
   "source": [
    "#tokenization\n",
    "# tok = spacy.load('en')\n",
    "tok = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mEJZH5iufZ66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:05, 8497.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in tqdm(reviews.iterrows()):\n",
    "    counts.update(tokenize(row['reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NcVjtiS-UsrR"
   },
   "outputs": [],
   "source": [
    "\n",
    "# with open('/content/drive/MyDrive/data/NLP_sentiment_analysis_data/count.pickle', 'wb') as outputfile:\n",
    "#   pickle.dump(counts,outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dkuyLk3DVbYf"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/MyDrive/data/NLP_sentiment_analysis_data/count.pickle', 'rb') as inputfile:\n",
    "#   counts=pickle.load(inputfile)\n",
    "# with open('count.pickle', 'rb') as inputfile:\n",
    "#   counts=pickle.load(inputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-QcG10alfZ67"
   },
   "outputs": [],
   "source": [
    "# #deleting infrequent words\n",
    "# print(\"num_words before:\",len(counts.keys()))\n",
    "# for word in list(counts):\n",
    "#     if counts[word] < 2:\n",
    "#         del counts[word]\n",
    "# print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SgUuFwYtfZ67"
   },
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SOunom1cfZ67"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "#     encoded = [0]*N\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "#     enc1 = [vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized]\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQuzQnOTfZ67",
    "outputId": "3b4b0635-a89f-4c41-ebfd-7bca7d4c4f69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:02<00:00, 21227.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "      <th>review_length</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This book was very informative, covering all a...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 19, 20, 16, 21, 22, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I didn't like this product it smudged all unde...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>[13, 31, 32, 33, 2, 34, 35, 36, 9, 37, 38, 39,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I simply love the product. I appreciate print ...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>[13, 42, 43, 23, 34, 7, 13, 44, 45, 46, 47, 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It goes on very easily and makes my eyes look ...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>[35, 50, 51, 5, 52, 19, 53, 38, 39, 54, 55, 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  ratings  review_length  \\\n",
       "0  This book was very informative, covering all a...        3             10   \n",
       "1  I am already a baseball fan and knew a bit abo...        4             23   \n",
       "2  I didn't like this product it smudged all unde...        0             14   \n",
       "3  I simply love the product. I appreciate print ...        4             13   \n",
       "4  It goes on very easily and makes my eyes look ...        4             13   \n",
       "\n",
       "                                             encoded  \n",
       "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 0, 0, ...  \n",
       "1  [13, 14, 15, 16, 17, 18, 19, 20, 16, 21, 22, 2...  \n",
       "2  [13, 31, 32, 33, 2, 34, 35, 36, 9, 37, 38, 39,...  \n",
       "3  [13, 42, 43, 23, 34, 7, 13, 44, 45, 46, 47, 48...  \n",
       "4  [35, 50, 51, 5, 52, 19, 53, 38, 39, 54, 55, 19...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews['encoded'] = reviews['reviews'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "\n",
    "reviews['encoded'] = reviews['reviews'].progress_apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "print(type(reviews['encoded']))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Q-6e2lbHfZ67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 6871, 4: 33193, 0: 4059, 1: 2265, 2: 3612})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how balanced the dataset is\n",
    "Counter(reviews['ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-IGQyguYfZ68"
   },
   "outputs": [],
   "source": [
    "X = list(reviews['encoded'])\n",
    "y = list(reviews['ratings'])\n",
    "\n",
    "# oversample = SMOTE()\n",
    "# X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "        \t\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMdT7QojfZ68"
   },
   "source": [
    "#### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vZaB3HMGfZ68"
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.X[idx]), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BAU_1-vsfZ68"
   },
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qjbgdFN-fZ68"
   },
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    criterion = FocalLoss(gamma=5)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y in tqdm(train_dl):\n",
    "            x = x.long().cuda()\n",
    "            y = y.long().cuda()\n",
    "            y_pred = model(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "        time.sleep(0.5)\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    criterion = FocalLoss(gamma=5)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y in valid_dl:\n",
    "        x = x.long().cuda()\n",
    "        y = y.long()\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat.detach().cpu(), y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred.cpu() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred.cpu(), y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pyTzIuRhfZ69"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0vwSEkqfZ6_"
   },
   "source": [
    "### Transformer with pretrained Glove word embeddings\n",
    "\n",
    "Download weights from : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9TbTKuQbfZ6_"
   },
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\"./glove.6B.50d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file) as f:\n",
    "        for line in tqdm(f):\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zFcawkCufZ6_"
   },
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in tqdm(word_counts):\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HQyp9tT4fZ7A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "336378it [00:14, 23429.29it/s]\n",
      "100%|██████████| 16191/16191 [00:00<00:00, 358322.39it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_path=\"./glove.6B.200d.txt\"\n",
    "Embed_size=200\n",
    "word_vecs = load_glove_vectors(glove_path)\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts,Embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Sbthr4KPfZ7A"
   },
   "outputs": [],
   "source": [
    "class Txf_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = True ## freeze embeddings\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, dropout=0.2,nhead=25)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.bn1=nn.BatchNorm1d(embedding_dim*70)\n",
    "        self.linear1 = nn.Linear(embedding_dim*70, 70)\n",
    "        self.dropout=nn.Dropout(p=0.2)\n",
    "        self.bn2=nn.BatchNorm1d(70)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(70, 5)\n",
    "        \n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mask = torch.eq(x,0)\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #mask = x.float().masked_fill(x == 0, float('-inf')).masked_fill(x !=0, float(0.0))\n",
    "        x=x.permute(1,0,2)\n",
    "        #print(x.shape)\n",
    "        x=self.transformer_encoder(x,src_key_padding_mask=mask)\n",
    "        x=x.permute(1,0,2)\n",
    "        #print(x.shape)\n",
    "        x=x.flatten(1)\n",
    "        #print(x.shape)\n",
    "        x=self.dropout(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.linear1(x)\n",
    "        x=self.bn2(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.linear2(x)\n",
    "        #print(x)\n",
    "        #x=self.softmax(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "l8f14VNMfZ7A"
   },
   "outputs": [],
   "source": [
    "model_glove = Txf_glove_vecs(vocab_size, Embed_size, pretrained_weights)\n",
    "model_glove=model_glove.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jac3UBRmfZ7A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:51<00:00, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.101, val loss 0.036, val accuracy 0.728, and val rmse 0.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 24.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.039, val loss 0.033, val accuracy 0.725, and val rmse 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.032, val loss 0.038, val accuracy 0.721, and val rmse 0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.027, val loss 0.028, val accuracy 0.740, and val rmse 0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.024, val loss 0.027, val accuracy 0.747, and val rmse 0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.021, val loss 0.027, val accuracy 0.747, and val rmse 0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.017, val loss 0.027, val accuracy 0.746, and val rmse 0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.015, val loss 0.030, val accuracy 0.736, and val rmse 0.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.012, val loss 0.033, val accuracy 0.747, and val rmse 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.010, val loss 0.030, val accuracy 0.740, and val rmse 0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:57<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.007, val loss 0.045, val accuracy 0.713, and val rmse 0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.006, val loss 0.048, val accuracy 0.731, and val rmse 0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 24.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.006, val loss 0.040, val accuracy 0.748, and val rmse 0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.004, val loss 0.053, val accuracy 0.721, and val rmse 0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.003, val loss 0.042, val accuracy 0.742, and val rmse 0.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:57<00:00, 24.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.002, val loss 0.056, val accuracy 0.745, and val rmse 0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.002, val loss 0.067, val accuracy 0.735, and val rmse 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:57<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.002, val loss 0.075, val accuracy 0.725, and val rmse 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.001, val loss 0.079, val accuracy 0.744, and val rmse 0.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.001, val loss 0.115, val accuracy 0.696, and val rmse 0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:57<00:00, 24.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.002, val loss 0.082, val accuracy 0.744, and val rmse 0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.001, val loss 0.103, val accuracy 0.742, and val rmse 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.001, val loss 0.120, val accuracy 0.710, and val rmse 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.001, val loss 0.076, val accuracy 0.725, and val rmse 0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:58<00:00, 23.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.001, val loss 0.106, val accuracy 0.739, and val rmse 0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:57<00:00, 24.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.000, val loss 0.114, val accuracy 0.732, and val rmse 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:57<00:00, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.000, val loss 0.125, val accuracy 0.735, and val rmse 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.000, val loss 0.137, val accuracy 0.700, and val rmse 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.001, val loss 0.151, val accuracy 0.723, and val rmse 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [00:59<00:00, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.000, val loss 0.144, val accuracy 0.743, and val rmse 0.830\n"
     ]
    }
   ],
   "source": [
    "train_model(model_glove, epochs=30, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model (model, test_dl):\n",
    "    model.eval()\n",
    "    pred=[]\n",
    "    y_true=[]\n",
    "    for x, y in tqdm(test_dl):\n",
    "        x = x.long().cuda()\n",
    "        y_true.append(y)\n",
    "        y_hat = model(x)\n",
    "        #loss = F.cross_entropy(y_hat.detach().cpu(), y)\n",
    "        predict = torch.max(y_hat, 1)[1]\n",
    "        predict=predict.cpu()\n",
    "        pred.append(predict)\n",
    "    #print(pred)\n",
    "    test_acc=accuracy_score(y_true,pred)\n",
    "    test_recall_score=recall_score(y_true,pred,average='weighted')\n",
    "    test_precision_score=precision_score(y_true,pred,average='weighted')\n",
    "    test_f1_score=f1_score(y_true,pred,average='weighted')\n",
    "    test_confusion_matrix=confusion_matrix(y_true,pred)\n",
    "                \n",
    "    print(\" Test accuracy is \"+str(test_acc))\n",
    "    print(\" test_recall_score is \"+str(test_recall_score))\n",
    "    print(\" test_precision_score is \"+str(test_precision_score))\n",
    "    print(\" test_f1_score is \"+str(test_f1_score))\n",
    "    print(test_confusion_matrix)  \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = pd.read_csv(\"gold_test.csv\")\n",
    "test_reviews = test_reviews[['reviews', 'ratings']]\n",
    "test_reviews.columns = ['reviews', 'ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 19999.79it/s]\n",
      "100%|██████████| 10000/10000 [00:48<00:00, 207.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test accuracy is 0.6607\n",
      " test_recall_score is 0.6607\n",
      " test_precision_score is 0.6447040887927425\n",
      " test_f1_score is 0.6519009182996106\n",
      "[[ 793  188  115   58  117]\n",
      " [ 216  124  144   54   92]\n",
      " [ 140   93  314  197  167]\n",
      " [  54   55  238  370  687]\n",
      " [  93   70  141  474 5006]]\n"
     ]
    }
   ],
   "source": [
    "#test_batch=len(test_reviews)\n",
    "test_batch=1\n",
    "test_reviews['ratings'] = test_reviews['ratings'].apply(lambda x: zero_numbering[x])\n",
    "test_reviews['encoded'] = test_reviews['reviews'].progress_apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "X_test = list(test_reviews['encoded'])\n",
    "y_test = list(test_reviews['ratings'])\n",
    "test_ds = ReviewsDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=test_batch,shuffle=False)\n",
    "test_model(model_glove, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_glove, \"./weights_only.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Txf_glove_vecs(\n",
       "  (embeddings): Embedding(16193, 200, padding_idx=0)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "    (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm1d(14000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=14000, out_features=70, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (bn2): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=70, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test=torch.load(\"./weights_only.pth\")\n",
    "model_test.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.weight : True\n",
      "encoder_layer.self_attn.in_proj_weight : True\n",
      "encoder_layer.self_attn.in_proj_bias : True\n",
      "encoder_layer.self_attn.out_proj.weight : True\n",
      "encoder_layer.self_attn.out_proj.bias : True\n",
      "encoder_layer.linear1.weight : True\n",
      "encoder_layer.linear1.bias : True\n",
      "encoder_layer.linear2.weight : True\n",
      "encoder_layer.linear2.bias : True\n",
      "encoder_layer.norm1.weight : True\n",
      "encoder_layer.norm1.bias : True\n",
      "encoder_layer.norm2.weight : True\n",
      "encoder_layer.norm2.bias : True\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight : True\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias : True\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight : True\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias : True\n",
      "transformer_encoder.layers.0.linear1.weight : True\n",
      "transformer_encoder.layers.0.linear1.bias : True\n",
      "transformer_encoder.layers.0.linear2.weight : True\n",
      "transformer_encoder.layers.0.linear2.bias : True\n",
      "transformer_encoder.layers.0.norm1.weight : True\n",
      "transformer_encoder.layers.0.norm1.bias : True\n",
      "transformer_encoder.layers.0.norm2.weight : True\n",
      "transformer_encoder.layers.0.norm2.bias : True\n",
      "transformer_encoder.layers.1.self_attn.in_proj_weight : True\n",
      "transformer_encoder.layers.1.self_attn.in_proj_bias : True\n",
      "transformer_encoder.layers.1.self_attn.out_proj.weight : True\n",
      "transformer_encoder.layers.1.self_attn.out_proj.bias : True\n",
      "transformer_encoder.layers.1.linear1.weight : True\n",
      "transformer_encoder.layers.1.linear1.bias : True\n",
      "transformer_encoder.layers.1.linear2.weight : True\n",
      "transformer_encoder.layers.1.linear2.bias : True\n",
      "transformer_encoder.layers.1.norm1.weight : True\n",
      "transformer_encoder.layers.1.norm1.bias : True\n",
      "transformer_encoder.layers.1.norm2.weight : True\n",
      "transformer_encoder.layers.1.norm2.bias : True\n",
      "transformer_encoder.layers.2.self_attn.in_proj_weight : True\n",
      "transformer_encoder.layers.2.self_attn.in_proj_bias : True\n",
      "transformer_encoder.layers.2.self_attn.out_proj.weight : True\n",
      "transformer_encoder.layers.2.self_attn.out_proj.bias : True\n",
      "transformer_encoder.layers.2.linear1.weight : True\n",
      "transformer_encoder.layers.2.linear1.bias : True\n",
      "transformer_encoder.layers.2.linear2.weight : True\n",
      "transformer_encoder.layers.2.linear2.bias : True\n",
      "transformer_encoder.layers.2.norm1.weight : True\n",
      "transformer_encoder.layers.2.norm1.bias : True\n",
      "transformer_encoder.layers.2.norm2.weight : True\n",
      "transformer_encoder.layers.2.norm2.bias : True\n",
      "transformer_encoder.layers.3.self_attn.in_proj_weight : True\n",
      "transformer_encoder.layers.3.self_attn.in_proj_bias : True\n",
      "transformer_encoder.layers.3.self_attn.out_proj.weight : True\n",
      "transformer_encoder.layers.3.self_attn.out_proj.bias : True\n",
      "transformer_encoder.layers.3.linear1.weight : True\n",
      "transformer_encoder.layers.3.linear1.bias : True\n",
      "transformer_encoder.layers.3.linear2.weight : True\n",
      "transformer_encoder.layers.3.linear2.bias : True\n",
      "transformer_encoder.layers.3.norm1.weight : True\n",
      "transformer_encoder.layers.3.norm1.bias : True\n",
      "transformer_encoder.layers.3.norm2.weight : True\n",
      "transformer_encoder.layers.3.norm2.bias : True\n",
      "transformer_encoder.layers.4.self_attn.in_proj_weight : True\n",
      "transformer_encoder.layers.4.self_attn.in_proj_bias : True\n",
      "transformer_encoder.layers.4.self_attn.out_proj.weight : True\n",
      "transformer_encoder.layers.4.self_attn.out_proj.bias : True\n",
      "transformer_encoder.layers.4.linear1.weight : True\n",
      "transformer_encoder.layers.4.linear1.bias : True\n",
      "transformer_encoder.layers.4.linear2.weight : True\n",
      "transformer_encoder.layers.4.linear2.bias : True\n",
      "transformer_encoder.layers.4.norm1.weight : True\n",
      "transformer_encoder.layers.4.norm1.bias : True\n",
      "transformer_encoder.layers.4.norm2.weight : True\n",
      "transformer_encoder.layers.4.norm2.bias : True\n",
      "transformer_encoder.layers.5.self_attn.in_proj_weight : True\n",
      "transformer_encoder.layers.5.self_attn.in_proj_bias : True\n",
      "transformer_encoder.layers.5.self_attn.out_proj.weight : True\n",
      "transformer_encoder.layers.5.self_attn.out_proj.bias : True\n",
      "transformer_encoder.layers.5.linear1.weight : True\n",
      "transformer_encoder.layers.5.linear1.bias : True\n",
      "transformer_encoder.layers.5.linear2.weight : True\n",
      "transformer_encoder.layers.5.linear2.bias : True\n",
      "transformer_encoder.layers.5.norm1.weight : True\n",
      "transformer_encoder.layers.5.norm1.bias : True\n",
      "transformer_encoder.layers.5.norm2.weight : True\n",
      "transformer_encoder.layers.5.norm2.bias : True\n",
      "bn1.weight : True\n",
      "bn1.bias : True\n",
      "linear1.weight : True\n",
      "linear1.bias : True\n",
      "bn2.weight : True\n",
      "bn2.bias : True\n",
      "linear2.weight : True\n",
      "linear2.bias : True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_test.named_parameters():\n",
    "    print(name, ':', param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5956, 0.0743, 0.0102, 0.2832, 0.0368]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[9.7568e-01, 2.0022e-02, 8.2854e-04, 3.0561e-04, 3.1663e-03]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.0019, 0.0063, 0.0558, 0.9095, 0.0265]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3925, 0.4195, 0.1645, 0.0181, 0.0054]], grad_fn=<SoftmaxBackward>)\n",
      "[tensor([1]), tensor([1]), tensor([4]), tensor([2])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_example_inp=[\"This product is not good\",\"this product is worst\",\"This product is ok\",\"This product is not ok\" ]\n",
    "test_out=[]\n",
    "for i in range(len(test_example_inp)):\n",
    "    test_example=torch.Tensor(encode_sentence(test_example_inp[i],vocab2index )).long().view(1,-1)\n",
    "    \n",
    "    test_prob=nn.functional.softmax(model_test.cpu()(test_example),dim=-1)\n",
    "    print(test_prob)\n",
    "    pred=(torch.max(test_prob, 1)[1])+1\n",
    "    test_out.append(pred)\n",
    "print(test_out)\n",
    "    \n",
    "\n",
    "  \n",
    "# test_example = (torch.Tensor(encode_sentence(test_example,vocab2index )).long()).view(len(test_example),-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(test_example_inp):\n",
    "    test_out=[]\n",
    "    for i in range(len(test_example_inp)):\n",
    "        test_example=torch.Tensor(encode_sentence(test_example_inp[i],vocab2index )).long().view(1,-1)\n",
    "        test_prob=(nn.functional.softmax(model_test.cpu()(test_example),dim=-1)).detach().numpy()\n",
    "        test_out.append(test_prob.reshape(-1))\n",
    "    return np.array(test_out)\n",
    "\n",
    "Result=prob([\"this is not good\",\"this is not ok\"])\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names=[0,1,2,3,4]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "exp = explainer.explain_instance(text_instance='This is not ok', classifier_fn=prob, num_features=3, labels=[4])\n",
    "print ('Explanation for class %s' % class_names[4])\n",
    "print ('\\n'.join(map(str, exp.as_list(label=4))))\n",
    "\n",
    "# This will explain for top 2 labels\n",
    "exp = explainer.explain_instance(text_instance='This is not ok',classifier_fn=prob, num_features=3, top_labels=2)\n",
    "print(exp.available_labels())\n",
    "exp.show_in_notebook(text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 pid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "YMdT7QojfZ68",
    "camjvpj5fZ69",
    "aHAn7853fZ6-",
    "z0vwSEkqfZ6_"
   ],
   "name": "LSTM_multiclass_text_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
